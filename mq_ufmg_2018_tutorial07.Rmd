# Tutorial 7 - Mais funções para análise de texto no R - o pacote _tidytext_
_Autor_: Leonardo Sangali Barone

_Revisão_: Rogério Jerônimo Barbosa

## Uma abordagem "tidy" para texto

Corpora são os objetos clássicos para processamento de linguagem natural. No R, porém, há uma tendência a deixar tudo "tidy". Vamos ver uma abordagem "tidy", ou seja, com data frames no padrão do _tidyverse_, para texto.

Vamos fazer uma rápida introdução, mas recomendo fortemente a leitura do livro [Text Mininig with R](http://tidytextmining.com/), disponível no formato "bookdown".

Comecemos carregando os seguintes pacotes (Se não possui algum deles, instale! E não se esqueça de instalar também as dependências com o argumento `dep = TRUE`):

```{r}
library(tidytext)
library(dplyr)
library(ggplot2)
library(tidyr)
```

Vamos usar o mesmo material do Tutorial anterior (discursos da Erundina), que já haviam sido também utilizados no Tutorial 5. Mas desta vez vamos tentar coletar todos os discursos. Na data de fechamento deste material (23/07/2018) eram 135 discursos. Como o site da câmara exibe apenas 50 resultados por página, devemos fazer 3 iterações com um loop para coletar primeiramente os links. Só depois passaremos ao loop que percorre cada um deles e raspa os conteúdos.

Como se trata de muito mais conteúdo, vai levar um tempinho para que a coleta seja concluída. Mas mãos à obra!

```{r}
library(rvest)   
library(stringr) 

# Substituimos o número da página por XXXXX
url_base <- "http://www.camara.leg.br/internet/sitaqweb/resultadoPesquisaDiscursos.asp?txIndexacao=&CurrentPage=XXXXX&BasePesq=plenario&txOrador=Luiza%20Erundina&txPartido=PSOL&dtInicio=&dtFim=&txUF=SP&txSessao=&listaTipoSessao=&listaTipoInterv=&inFalaPres=&listaTipoFala=&listaFaseSessao=&txAparteante=&listaEtapa=&CampoOrdenacao=dtSessao&TipoOrdenacao=DESC&PageSize=50&txTexto=&txSumario="

url_discursos <- NULL
for(num_pag in 1:3){
        
        print(num_pag)
        
        url <- str_replace(url_base, "XXXXX", as.character(num_pag))
        
        url_discursos_tmp <- url %>%
                read_html() %>%
                html_nodes(xpath = "//a[contains(@title, 'do Discurso')]") %>%
                html_attr(name = "href") %>%
                str_c("http://www.camara.leg.br/internet/sitaqweb/", .) %>%
                str_replace_all("[[:space:]]", "")
        
        url_discursos = c(url_discursos, url_discursos_tmp)
}


discursos <- c()
for (i in 1:length(url_discursos)) {
  print(i)
  url_discurso <- url_discursos[i]
        
  discurso <- url_discurso %>%
    read_html() %>%
    html_nodes(xpath = "//div[@id  = 'content']//p") %>%
    html_text()
  
  discursos <- c(discursos, discurso)
  Sys.sleep(0.3) 
}
```

Vamos recriar o data frame com os discursos:

```{r}
discursos_df <- data_frame(doc_id = 1:length(discursos), 
                           text   = discursos)
glimpse(discursos_df)
```

Criar data.frames... esse é o segredo da abordagem _tidy_ da análise de textos. Não vamos trabalhar com um objeto mais complexo (e até um pouco estranho...) que são os `corpus` do pacote _tm_, tal como havíamos feito no tutorial anterior. 

Utilizamos bancos de dados para a maioria de nossas análises, certo? A organização tabular (em linhas e colunas) é basicamente o padrão quando realizamos análises estatísticas. Bem... o pacote _tidytext_ visa exatamente nos manter nesse território conhecido, para o qual temos ferramentas também conhecidas e mais amplas.


### Tokens

A primeira função interessante do pacote _tidytext_ é justamente a tokenização de um texto (lembra o que é isso? dê uma relembrada, olhando o Tutorial anterior). Veja como funciona:

```{r}
discursos_token <- discursos_df %>%
  unnest_tokens(output = word, input = text) # nome das variáveis de saida (a ser criada) e entrada (nossos textos)
glimpse(discursos_token)
```

Note que a variável _doc\_id_, criada por nós, é mantida. "text", porém, se torna "words", na exata sequência do texto. Veja que o formato de um "tidytext" é completamnte diferente de um Corpus.

Como excluir stopwords nessa abordagem? Já adiantamos que os data frames são a chave da abordagem tidy, certo? Então, é isso! **Precisamos de um data frame com stopwords**! Vamos recriar um vetor stopwords_pt, que é a versão ampliada das stopwords disponíveis no R, e criar um data frame com tal vetor:

```{r}
stopwords_pt <- c(stopwords("pt"), "presidente", "é", "sr", "sra", "luiza", 
                  "erundina", "oradora", "revisão", "sp", "v.exa")
stopwords_pt_df <- data_frame(word = stopwords_pt)
```

Com _anti\_join_ (Já viu essa função? Ela é do pacote dplyr. Procure-a no _help_!) mantemos em "discursos\_token" apenas as palavras que não estao em "stopwords\_pt\_df"

```{r}
discursos_token <- discursos_token %>%
  anti_join(stopwords_pt_df, by = "word")
```

Para observarmos a frequência de palavras nos discursos, usamos _count_, do pacote _dplyr_:

```{r}
discursos_token %>%
  count(word, sort = TRUE)
```

Com _ggplot_, podemos construir um gráfico de barras dos temos mais frequêntes, por exemplo, com frequência maior do que 50. Neste ponto do curso, nada do que estamos fazendo abaixo deve ser novo a você:

```{r}
discursos_token %>%
  count(word, sort = TRUE) %>%
  filter(n > 50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```

Incorporando a função _wordcloud_ a nossa análise:

```{r}
library(wordcloud)

discursos_token %>%
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, max.words = 50, scale = c(3, .4)))
```

A abordagem "tidy" para texto nos mantém no território confortável da manipulação de data frames e, particularmente, me parece mais atrativa do que a abordagem via Corpus para um conjunto grande de casos.

### Bigrams

Já produzimos duas vezes a tokenização do texto, sem, no entanto, refletir sobre esse procedimento. Tokens são precisam ser formados por palavras únicas. Se o objetivo for, por exemplo, observar a ocorrência conjunta de termos, convém trabalharmos com bigrams (tokens de 2 palavras) ou ngrams (tokens de n palavras). Vejamos como:

```{r}
discurso_bigrams <- discursos_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

Veja com fica:

```{r}
print(discurso_bigrams)
```

Note que, ao tokenizar o texto, automaticamente foram excluídas as as pontuações e as palavras foram alteradas para minúscula (use o argumento "to_lower = FALSE" caso não queira a conversão). Vamos contar os bigrams:

```{r}
discurso_bigrams %>%
  count(bigram, sort = TRUE)
```

Como, porém, excluir as stopwords quando elas ocorrem em bigrams? Em primeiro, temos que separar os bigrams e duas palavras, uma em cada coluna:

```{r}
bigrams_separated <- discurso_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
```

E, a seguir, filter o data frame excluindo as stopwords (note que aproveitamos o vetor "stopwords_pt"):

```{r}
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stopwords_pt) %>%
  filter(!word2 %in% stopwords_pt)
```

ou, usando _anti\_join_, como anteriormente:

```{r}
bigrams_filtered <- bigrams_separated %>%
  anti_join(stopwords_pt_df, by = c("word1" = "word")) %>%
  anti_join(stopwords_pt_df, by = c("word2" = "word"))
```

Produzindo a frequência de bigrams:

```{r}
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
```

Reunindo as palavras do bigram que foram separadas para excluirmos as stopwords:

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
```

A abordagem "tidy" traz uma tremenda flexibilidade. Se, por exemplo, quisermos ver com quais palavras a palavra "casa" é antecedida:

```{r}
bigrams_filtered %>%
  filter(word2 == "casa") %>%
  count(word1, sort = TRUE)
```

Ou precedida:

```{r}
bigrams_filtered %>%
  filter(word1 == "poder") %>%
  count(word2, sort = TRUE)
```

Ou ambos:

```{r}
bf1 <- bigrams_filtered %>%
  filter(word2 == "poder") %>%
  count(word1, sort = TRUE) %>%
  rename(word = word1)

bf2 <- bigrams_filtered %>%
  filter(word1 == "poder") %>%
  count(word2, sort = TRUE) %>%
  rename(word = word2)

bind_rows(bf1, bf2) %>%
  arrange(-n)
```

Super simples e legal, não?

### Ngrams

Repetindo o procedimento para "trigrams":

```{r}
discursos_df %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  anti_join(stopwords_pt_df, by = c("word1" = "word")) %>%
  anti_join(stopwords_pt_df, by = c("word2" = "word")) %>%
  anti_join(stopwords_pt_df, by = c("word3" = "word")) %>%
  count(word1, word2, word3, sort = TRUE)
```

### Redes de palavras

Para encerrar, vamos a um dos usos mais interessantes do ngrams: a construção de redes de palavras. Precisaremos de dois novos pacotes, _igraph_ e _ggraph_. Instale-os se precisar:

```{r}
library(igraph)
library(ggraph)
```

Em primeiro lugar, transformaremos nosso data frame em um objeto da classe _igraph_, do pacote de mesmo nome, usado para a presentação de redes no R:

```{r}
bigram_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()
```

A seguir, com o pacote _ggraph_, faremos o grafo a partir dos bigrams dos discursos da deputada:

```{r}
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

Note que são formadas pequenas associações entre termos que, par a par, caminham juntos. Novamente, não vamos explorar aspectos analíticos da mineração de texto, mas estas associações são informações de grande interessa a depender dos objetivos da análise.

## Para além do tutorial

No tutorial, vimos o básico da preparação de textos para mineração, como organizar um Corpus e criar tokens. Além disso, vimos várias utilidades do pacote _stringr_, que serve para além da mineração de texto e pode ser útil na organização de bases de dados que contém variáveis "character".

Se houver tempo em sala de aula e você quiser se aprofundar no assunto, leia alguns dos capítulos de [Text Mininig with R](http://tidytextmining.com/):

- [Capítulo 2 - Análise de Sentimento (com textos em inglês)](http://tidytextmining.com/sentiment.html)

- [Capítulo 3 - Análise de frequência de palavras](http://tidytextmining.com/tfidf.html)

- [Capítulo 4 - Relacionamento entre palavras, n-gramas e correlação](http://tidytextmining.com/ngrams.html)

- [Capítulo 6 - Topic Modeling](http://tidytextmining.com/topicmodeling.html)
